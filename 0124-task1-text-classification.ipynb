{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95a04534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb7de303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '/Users/zhanghan/mygithub/nlp-beginner/task1-sentiment-analysis-on-movie-reviews/train.tsv'\n",
    "# df = load_data(data_path)\n",
    "# sentences = df['Phrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4d81a",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "605bcb59",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1\n",
      "  'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'\n",
      "  1]\n",
      " [2 1\n",
      "  'A series of escapades demonstrating the adage that what is good for the goose'\n",
      "  2]\n",
      " [3 1 'A series' 2]\n",
      " [4 1 'A' 2]\n",
      " [5 1 'series' 2]]\n",
      "---\n",
      "[[1 1\n",
      "  'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'\n",
      "  1]\n",
      " [2 1\n",
      "  'A series of escapades demonstrating the adage that what is good for the goose'\n",
      "  2]]\n",
      "---\n",
      "A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\n"
     ]
    }
   ],
   "source": [
    "data_t = np.array(pd.read_csv('/Users/zhanghan/mygithub/nlp-beginner/task1-sentiment-analysis-on-movie-reviews/train.tsv',\n",
    "                            sep='\\t', header=0, nrows=5))\n",
    "# X_text = df_t[?]#取出phrase列\n",
    "\n",
    "print(df_t)\n",
    "print('---')\n",
    "print(sentences_t)\n",
    "# print(len(sentences_t))\n",
    "print('---')\n",
    "print(data_t[0][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecccdf",
   "metadata": {},
   "source": [
    "## load data (fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "369935c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    '''load data from tsv file'''\n",
    "    df = pd.read_csv(data_path, sep='\\t', header=0, index_col='id')\n",
    "    return np.array(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864c108",
   "metadata": {},
   "source": [
    "## visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bdf3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(data):#还可以改改 或者不写也行\n",
    "    '''visualize original data'''\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738f2e6",
   "metadata": {},
   "source": [
    "## clean text (fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e407dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', 'a series escapades demonstrating', '1'],\n",
       "       ['2', '1', 'a series escapades demonstrating adage good goose',\n",
       "        '2'],\n",
       "       ['3', '1', 'a series', '2'],\n",
       "       ['4', '1', 'a', '2'],\n",
       "       ['5', '1', 'series', '2']], dtype='<U77')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopword(data_orig):\n",
    "    '''\n",
    "    remove stopwords in the wordlist\n",
    "    \n",
    "    data: array\n",
    "    return an array\n",
    "    '''\n",
    "    data_filtered = data_orig.copy()   \n",
    "    for i in range(data_orig.shape[0]):# 通过遍历修改data[i][2]的字符串\n",
    "        filtered_words = [word.lower() for word in data_orig[i][2].split() if word not in stopwords.words('english')]\n",
    "        l = ' '.join(filtered_words)\n",
    "        data_filtered[i][2] = ' '.join(filtered_words) \n",
    "    return data_filtered\n",
    "\n",
    "\n",
    "# test\n",
    "d = np.array([[1,1,'A series of escapades demonstrating the a',1],\n",
    "               [2,1, 'A series of escapades demonstrating the adage that what is good for the goose', 2],\n",
    "               [3, 1, 'A series' ,2],\n",
    "               [4, 1, 'A', 2],\n",
    "               [5, 1, 'series', 2]])\n",
    "d_f = remove_stopword(d)\n",
    "d_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bbd3ab",
   "metadata": {},
   "source": [
    "## make dictionary=tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b64ec2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之后要把这里的data改成X，不要整个data进入\n",
    "def make_dict_bow(data_filtered, text_col=2):\n",
    "    '''\n",
    "    make a bow dictionary based on all the words in the text\n",
    "    return a set that contains a word list mapped with numbers\n",
    "    \n",
    "    data: filtered data with all columns\n",
    "    text_col: text column index\n",
    "    '''\n",
    "    # make a non-repetitive word set\n",
    "    word_set = set()\n",
    "    for row in data_filtered:\n",
    "        for word in row[text_col].split():\n",
    "            word_set.add(word) # ('a','b')\n",
    "            \n",
    "    # map each string with an index number\n",
    "    word_size = len(word_set)\n",
    "    dict_bow = dict(zip(word_set,range(word_size))) # {'a':0,'b':1}\n",
    "                    \n",
    "    return dict_bow\n",
    "\n",
    "                    \n",
    "def make_dict_ng(data_filtered, text_col=2, ngram = 3):\n",
    "    '''\n",
    "    make a n-gram dict\n",
    "    \n",
    "    data_filtered: sentences array\n",
    "    ngram: n-gram number\n",
    "    '''\n",
    "    # \"在一般的文本分类任务中,N 取 3 就足够了\n",
    "    \n",
    "    # make a non-repetitive word set \n",
    "    word_set = set()\n",
    "    for row in data_filtered:\n",
    "        wordlist_row = row[text_col].split()\n",
    "        for n in range(ngram):\n",
    "            for i in range(len(wordlist_row) - n):\n",
    "                word_set.add(' '.join(wordlist_row[ i : i + ngram ])) # ('a','a b')\n",
    "        \n",
    "    # map each string with an index number\n",
    "    word_size = len(word_set)\n",
    "    dict_ng = dict(zip(word_set,range(word_size))) # {'a':0,'b':1}\n",
    "    \n",
    "    return dict_ng\n",
    "\n",
    "# test passed!\n",
    "# d2 = np.array([['1', '1', 'a series escapades demonstrating', '1'],\n",
    "#        ['2', '1', 'a series escapades demonstrating adage good goose',\n",
    "#         '2'],\n",
    "#        ['3', '1', 'a series', '2'],\n",
    "#        ['4', '1', 'a', '2'],\n",
    "#        ['5', '1', 'series', '2']], dtype='<U77')\n",
    "# print('d2_bow: ', make_dict_bow(d2,2))\n",
    "# print('d2_ng: ', make_dict_ng(d2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553b9b1",
   "metadata": {},
   "source": [
    "## data -> vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa022bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: \n",
      " [['1' '1' 'a series escapades demonstrating' '1']\n",
      " ['2' '1' 'a series escapades demonstrating adage good goose' '2']\n",
      " ['3' '1' 'a series' '2']\n",
      " ['4' '1' 'a' '2']\n",
      " ['5' '1' 'series' '2']] \n",
      "\n",
      "{'adage': 0, 'demonstrating': 1, 'good': 2, 'escapades': 3, 'a': 4, 'goose': 5, 'series': 6} \n",
      " ************************* \n",
      " [[0. 1. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "\n",
      "{'a series': 0, 'demonstrating': 1, 'good goose': 2, 'series escapades demonstrating': 3, 'a': 4, 'a series escapades': 5, 'goose': 6, 'series': 7, 'escapades demonstrating': 8, 'adage good goose': 9, 'escapades demonstrating adage': 10, 'demonstrating adage good': 11} \n",
      " ************************* \n",
      " [[1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 1., 1., 0., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize_bow(data_filtered, dict_bow, text_col=2): \n",
    "    '''\n",
    "    data_filtered: array, filtered data\n",
    "    return an array of vectorized features\n",
    "    '''\n",
    "    features_bow = np.zeros((data_filtered.shape[0],len(dict_bow)))\n",
    "    for i in range(data_filtered.shape[0]): # array中的每个句子：\n",
    "        phrase_i = data_filtered[i][text_col]\n",
    "        wordlist = phrase_i.split()\n",
    "        for word in wordlist: # for每个句子中的每个单词：\n",
    "            if word in dict_bow:# if这个单词在字典中：\n",
    "                j = dict_bow[word]\n",
    "                features_bow[i][j] += 1 # features对应dict相同index的位置变为1\n",
    "    return features_bow\n",
    "\n",
    "\n",
    "def vectorize_ng(data_filtered, dict_ng, text_col=2, ngram=3): # N-gram：ng_vectorizer.transform(corpus)\n",
    "    '''\n",
    "    data_filtered: array, filtered data\n",
    "    return an array of vectorized features\n",
    "    '''\n",
    "    features_ng = np.zeros((data_filtered.shape[0],len(dict_ng)))\n",
    "    for i in range(data_filtered.shape[0]): # for array中的每个句子\n",
    "        phrase_i = data_filtered[i][text_col]\n",
    "        wordlist = phrase_i.split()\n",
    "        for n in range(ngram):\n",
    "            for index in range(len(wordlist)-n): # for每个句子中的每个word_ng：\n",
    "                word_ng = ' '.join(wordlist[ index : index + n+1 ])\n",
    "                # print(index, word_ng)\n",
    "                if word_ng in dict_ng: # if这个word_ng在字典中：\n",
    "                    j = dict_ng[word_ng]\n",
    "                    features_ng[i][j] += 1 # features对应dict相同index的位置+1\n",
    "    return features_ng\n",
    "\n",
    "# test passed!\n",
    "a3 = np.array([['1', '1', 'a series escapades demonstrating', '1'],\n",
    "       ['2', '1', 'a series escapades demonstrating adage good goose',\n",
    "        '2'],\n",
    "       ['3', '1', 'a series', '2'],\n",
    "       ['4', '1', 'a', '2'],\n",
    "       ['5', '1', 'series', '2']], dtype='<U77')\n",
    "print('original data: \\n',a3,'\\n')\n",
    "\n",
    "dict_bow_t = make_dict_bow(a3,2)\n",
    "dict_ng_t = make_dict_ng(a3)\n",
    "\n",
    "f_t_bow = vectorize_bow(a3, dict_bow_t)\n",
    "f_t_ng = vectorize_ng(a3, dict_ng_t)\n",
    "print(dict_bow_t, '\\n', '*'*25, '\\n',f_t_bow)\n",
    "print('\\n')\n",
    "print(dict_ng_t, '\\n', '*'*25, '\\n', f_t_ng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58efae5",
   "metadata": {},
   "source": [
    "## train (get model+gradient descent)\n",
    "\n",
    "cost function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{R}(w)\n",
    "\t&=-\\frac{1}{N}\\displaystyle \\sum^{N}_{n=1}\\mathcal{L}(y^{(n)},\\hat y^{(n)})\\\\\n",
    "\t&=-\\frac{1}{N}\\displaystyle \\sum^{N}_{n=1}{\\left(y^{(n)}\\right)^T\\log \\hat y^{(n)}\\\\}\n",
    "\\end{aligned}\\tag{3.6}\n",
    "$$\n",
    "\n",
    "gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{R}(w)}{\\partial w}=-\\frac{1}{N}\\sum_{n=1}^{N}{x^{(n)}(y^{(n)}-\\hat y^{(n)})}\\tag{3.7}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c8fbe71",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,) (7,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m y_train_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([d_f[i, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X_train_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])], dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    103\u001b[0m W_init \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n,))\n\u001b[0;32m--> 104\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmini-batch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X_train, y_train, W_init, b_init, epochs, alpha, gd)\u001b[0m\n\u001b[1;32m     87\u001b[0m b_init \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m-\u001b[39m alpha \u001b[38;5;241m*\u001b[39m dj_db\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Save cost at each iteration (of a batch)\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m cost_hist\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcompute_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_init\u001b[49m\u001b[43m)\u001b[49m )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# print cost at an interval of 1000 iterations\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36mcompute_cost\u001b[0;34m(X, y, W, b)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mcompute the total cost of given parameters\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    W (array, (n,))\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     18\u001b[0m m,n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 19\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[1;32m     20\u001b[0m cost \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(y, np\u001b[38;5;241m.\u001b[39mlog(y_hat)) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cost\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,) (7,) "
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    '''compute the softmax output of z = wx + b'''\n",
    "    ez = np.exp(z)\n",
    "    sm = ez / np.sum(ez)\n",
    "    return sm\n",
    "    \n",
    "\n",
    "def compute_cost(X, y, W, b):\n",
    "    '''\n",
    "    compute the total cost of given parameters\n",
    "    \n",
    "    Args:\n",
    "        X (array, (m,n)): m training examples, n features\n",
    "        y (array, (m,))\n",
    "        W (array, (n,))\n",
    "    '''\n",
    "    \n",
    "    m,n = X.shape\n",
    "    y_hat = np.dot(X,W) + b\n",
    "    cost = np.dot(y, np.log(y_hat)) * (-1) / m\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def train(X_train, y_train, W_init, b_init, epochs, alpha, gd='mini-batch'):\n",
    "    '''\n",
    "    learn the parameters\n",
    "    \n",
    "    Args:\n",
    "        X_train (array, (m,n)): m for training examples, n for features\n",
    "        y_train (array, (m,))\n",
    "        W_init (array, (n,))\n",
    "        alpha (float): learning rate\n",
    "        epochs (int): \n",
    "        gd (string): method for gradient descent. 'batch', 'stochastic' or 'mini-batch'\n",
    "    Return\n",
    "        W (array, (n,)): parameters\n",
    "        cost_hist (list): a list of cost for plotting\n",
    "    '''\n",
    "    \n",
    "    m,n = X_train.shape\n",
    "    cost_hist = []\n",
    "    W = copy.deepcopy(W_init)\n",
    "    \n",
    "    \n",
    "    if gd == 'batch':\n",
    "        batch_size = m # 每个batch里的example数\n",
    "        itertimes = 1\n",
    "    if gd == 'stochastic':\n",
    "        batch_size = 1\n",
    "        itertimes = m\n",
    "    if gd == 'mini-batch':\n",
    "        batch_size = 64 # temp\n",
    "        itertimes = m // batch_size + 1 # 1 for the non-full batch\n",
    "        \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        #shuffle\n",
    "        arr_XY = list(zip(X_train, y_train))\n",
    "        np.random.shuffle(arr_XY)\n",
    "        X_sf, y_sf = zip(*arr_XY)\n",
    "        X_sf, y_sf = np.array(X_sf),np.array(y_sf)\n",
    "        \n",
    "        # update parameters after iterating over all exmpls in one batch. mini-batch by default.\n",
    "        # BGD iterates over all exmpls at one time; SGD iterates over one exmpl at a time\n",
    "        for k in range(itertimes):# for every batch（bgd has one batch, sgd has m ones, mini has itertimes batches）\n",
    "\n",
    "            # compute gradient\n",
    "            dj_dw = np.zeros((n,))                            #(n,)\n",
    "            dj_db = 0.                                        #scalar\n",
    "            if k == itertimes - 1: # if the last batch\n",
    "                batch_size = m % 64 # temp\n",
    "            for i in range(batch_size):# for every training example in the batch\n",
    "                i_batch = i + k * batch_size\n",
    "                y_hat_i = softmax(np.dot(X_sf[i_batch],W)+b_init)       #(n,)(n,)=scalar\n",
    "                err_i  = y_hat_i  - y_sf[i_batch]                       #scalar\n",
    "                # print('W shape:',W.shape)\n",
    "                # print(type(y_hat_i))\n",
    "                for j in range(n):\n",
    "                    dj_dw[j] = dj_dw[j] + err_i * X_sf[i_batch,j]       #scalar\n",
    "                dj_db += err_i\n",
    "            dj_dw = dj_dw/m                                             #(n,)       \n",
    "            dj_db = dj_db/m  \n",
    "\n",
    "            # gradient descent\n",
    "            W = W - alpha * dj_dw\n",
    "            b_init = W - alpha * dj_db\n",
    "            \n",
    "            # Save cost at each iteration (of a batch)\n",
    "            cost_hist.append(compute_cost(X_sf, y_sf, W, b_init) )\n",
    "            \n",
    "            # print cost at an interval of 1000 iterations\n",
    "            if k % 1000 == 0:\n",
    "                print(f\"Iteration {k:4}: Cost {float(cost_hist[-1]):8.2f}\")\n",
    "                \n",
    "    return W, b_init, cost_hist\n",
    "\n",
    "\n",
    "# test\n",
    "m,n = X_train_t.shape\n",
    "X_train_t = f_t_bow\n",
    "y_train_t = np.array([d_f[i, 3] for i in range(X_train_t.shape[0])], dtype = int)\n",
    "W_init = np.zeros((n,))\n",
    "train(X_train_t, y_train_t, W_init, b_init=0, epochs=2, alpha=0.1, gd='mini-batch')\n",
    "\n",
    "#接下来确认一下compute cost里的维度\n",
    "# w_t, b_t, cost_hist_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af30a9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m X_train_t \u001b[38;5;241m=\u001b[39m f_t_bow\n\u001b[1;32m      4\u001b[0m y_train_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([d_f[i, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X_train_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])], dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmini-batch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X_train, y_train, W_init, b_init, epochs, alpha, gd)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28mprint\u001b[39m(n)\n\u001b[0;32m---> 78\u001b[0m         \u001b[43mdj_dw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m dj_dw[j] \u001b[38;5;241m+\u001b[39m err_i \u001b[38;5;241m*\u001b[39m X_sf[i_batch,j]       \u001b[38;5;66;03m#scalar\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     dj_db \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m err_i\n\u001b[1;32m     80\u001b[0m dj_dw \u001b[38;5;241m=\u001b[39m dj_dw\u001b[38;5;241m/\u001b[39mm                                             \u001b[38;5;66;03m#(n,)       \u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "177030e6",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cb82860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([2, 2, 2]), 'c'),\n",
       " (array([1, 1, 1]), 'b'),\n",
       " (array([0, 0, 0]), 'a'),\n",
       " (array([4, 4, 4]), 'e'),\n",
       " (array([3, 3, 3]), 'd')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a_x = np.array([[0,0,0],[1,1,1],[2,2,2],[3,3,3],[4,4,4]])\n",
    "a_y = np.array(['a','b','c','d','e'])\n",
    "\n",
    "a_xy = list(zip(a_x,a_y))\n",
    "np.random.shuffle(a_xy)\n",
    "a_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data():\n",
    "\n",
    "\n",
    "\n",
    "def accuracy():\n",
    "\n",
    "def plot_result():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
