{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2bc112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3911a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '/Users/zhanghan/mygithub/nlp-beginner/task1-sentiment-analysis-on-movie-reviews/train.tsv'\n",
    "# df = load_data(data_path)\n",
    "# sentences = df['Phrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22aee58",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bdba3306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1\n",
      "  'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'\n",
      "  1]\n",
      " [2 1\n",
      "  'A series of escapades demonstrating the adage that what is good for the goose'\n",
      "  2]\n",
      " [3 1 'A series' 2]\n",
      " [4 1 'A' 2]\n",
      " [5 1 'series' 2]]\n",
      "---\n",
      "[[1 1\n",
      "  'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'\n",
      "  1]\n",
      " [2 1\n",
      "  'A series of escapades demonstrating the adage that what is good for the goose'\n",
      "  2]]\n",
      "---\n",
      "A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\n"
     ]
    }
   ],
   "source": [
    "data_t = np.array(pd.read_csv('/Users/zhanghan/mygithub/nlp-beginner/task1-sentiment-analysis-on-movie-reviews/train.tsv',\n",
    "                            sep='\\t', header=0, nrows=5))\n",
    "# sentences_t = df_t[?]#取出phrase列\n",
    "\n",
    "print(df_t)\n",
    "print('---')\n",
    "print(sentences_t)\n",
    "# print(len(sentences_t))\n",
    "print('---')\n",
    "print(data_t[0][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa35b291",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f190bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    '''load data from tsv file'''\n",
    "    df = pd.read_csv(data_path, sep='\\t', header=0, index_col='id')\n",
    "    return np.array(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7098d0",
   "metadata": {},
   "source": [
    "## visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c16f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(data):#还可以改改 或者不写也行\n",
    "    '''visualize original data'''\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a4405",
   "metadata": {},
   "source": [
    "## clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2c3d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "\n",
    "\n",
    "def remove_stopword(data):\n",
    "    '''\n",
    "    remove stopwords in the wordlist\n",
    "    \n",
    "    data: array\n",
    "    >>>data = [[1 1 'A series of escapades demonstrating the a' 1]\n",
    "               [2 1 'A series of escapades demonstrating the adage that what is good for the goose' 2]\n",
    "               [3 1 'A series' 2]\n",
    "               [4 1 'A' 2]\n",
    "               [5 1 'series' 2]]\n",
    "    >>>remove_stopword(sentences)\n",
    "    ['sss.', 'bbb', 'aaa', 'A', 'ddd.']\n",
    "    '''\n",
    "#     filtered_sentences = [] \n",
    "#     for phrase in list(sentences):\n",
    "#         word_list_t = phrase.split()\n",
    "#         filtered_words = [word for word in word_list_t if word not in stopwords.words('english')]\n",
    "#         filtered_sentences.append(' '.join(filtered_words))\n",
    "        \n",
    "    for i in range(data.shape[0]):#通过遍历修改data[i][2]的字符串\n",
    "#         print(\"before filtering: \",data[i][2])\n",
    "        data_filtered = data.deepcopy()\n",
    "        filtered_words = [word for word in data[i][2].split() if word not in stopwords.words('english')]\n",
    "        data_filtered[i][2] = ' '.join(filtered_words) # string\n",
    "#         print(\"after filtering: \",data2[i][2])\n",
    "    return data_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7410c",
   "metadata": {},
   "source": [
    "## make dictionary=tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35befc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict_bow(data): #普通地写词典\n",
    "    '''\n",
    "    make a bow dictionary based on all the words in the text\n",
    "    return a set that contains a word list mapped with numbers\n",
    "    \n",
    "    data: \n",
    "    '''\n",
    "    # make a non-repetitive word set\n",
    "    word_set = set()\n",
    "    for row in data:\n",
    "        for word in row.split():\n",
    "            word = word.lower()\n",
    "            word_set.add(word) # ('a','b')\n",
    "            \n",
    "    # map each string with an index number\n",
    "    word_size = len(word_set)\n",
    "    dict_bow = dict(zip(word_set,range(word_size))# {'a':0,'b':1}\n",
    "                    \n",
    "    return dict_bow\n",
    "\n",
    "                    \n",
    "def make_dict_ng(data): #N-gram参考下别人的 ng_vectorizer.fit(corpus)\n",
    "    '''\n",
    "    make a n-gram dict\n",
    "    '''\n",
    "    \n",
    "    return dict_ng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f6c84",
   "metadata": {},
   "source": [
    "## data -> vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1130426",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '：' (U+FF1A) (2817608247.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [8]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def vectorize_bow(): BoW：bog_vectorizer.transform(corpus) 类似这个的函数写法（搞清楚它是怎么写的就行\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '：' (U+FF1A)\n"
     ]
    }
   ],
   "source": [
    "def vectorize_bow(): BoW：bog_vectorizer.transform(corpus) 类似这个的函数写法（搞清楚它是怎么写的就行\n",
    "    \n",
    "    return 1\n",
    "\n",
    "\n",
    "def vectorize_ng(): N-gram：ng_vectorizer.transform(corpus)\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a29a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "后面参考ng就可以 和那个识别10011的task一样\n",
    "\n",
    "### #train (get model+gradient descent)\n",
    "\n",
    "对于softmax和logistic各自执行以下几个：\n",
    "\n",
    "def compute_cost(): (计算代价函数/经验风险)\n",
    "\n",
    "def compute_gradient():\n",
    "\n",
    "def perform_desc(): shuffle batch minibatch\n",
    "\n",
    "\n",
    "\n",
    "这时应该学习好了参数\n",
    "\n",
    "### #prediction\n",
    "\n",
    "\n",
    "\n",
    "### #save results; accuracy; plot results\n",
    "\n",
    "def save_data():\n",
    "\n",
    "\n",
    "\n",
    "def accuracy():\n",
    "\n",
    "def plotr_result():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416e055",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
