{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95a04534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb7de303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '/Users/zhanghan/mygithub/nlp-beginner/task1-sentiment-analysis-on-movie-reviews/train.tsv'\n",
    "# df = load_data(data_path)\n",
    "# sentences = df['Phrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4d81a",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "605bcb59",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1\n",
      "  'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'\n",
      "  1]\n",
      " [2 1\n",
      "  'A series of escapades demonstrating the adage that what is good for the goose'\n",
      "  2]\n",
      " [3 1 'A series' 2]\n",
      " [4 1 'A' 2]\n",
      " [5 1 'series' 2]]\n",
      "---\n",
      "[[1 1\n",
      "  'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'\n",
      "  1]\n",
      " [2 1\n",
      "  'A series of escapades demonstrating the adage that what is good for the goose'\n",
      "  2]]\n",
      "---\n",
      "A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\n"
     ]
    }
   ],
   "source": [
    "data_t = np.array(pd.read_csv('/Users/zhanghan/mygithub/nlp-beginner/task1-sentiment-analysis-on-movie-reviews/train.tsv',\n",
    "                            sep='\\t', header=0, nrows=5))\n",
    "# X_text = df_t[?]#取出phrase列\n",
    "\n",
    "print(df_t)\n",
    "print('---')\n",
    "print(sentences_t)\n",
    "# print(len(sentences_t))\n",
    "print('---')\n",
    "print(data_t[0][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecccdf",
   "metadata": {},
   "source": [
    "## load data (fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "369935c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    '''load data from tsv file'''\n",
    "    df = pd.read_csv(data_path, sep='\\t', header=0, index_col='id')\n",
    "    return np.array(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864c108",
   "metadata": {},
   "source": [
    "## visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bdf3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(data):#还可以改改 或者不写也行\n",
    "    '''visualize original data'''\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738f2e6",
   "metadata": {},
   "source": [
    "## clean text (fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e407dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '1', 'a series escapades demonstrating', '1'],\n",
       "       ['2', '1', 'a series escapades demonstrating adage good goose',\n",
       "        '2'],\n",
       "       ['3', '1', 'a series', '2'],\n",
       "       ['4', '1', 'a', '2'],\n",
       "       ['5', '1', 'series', '2']], dtype='<U77')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopword(data_orig):\n",
    "    '''\n",
    "    remove stopwords in the wordlist\n",
    "    \n",
    "    data: array\n",
    "    return an array\n",
    "    '''\n",
    "    data_filtered = data_orig.copy()   \n",
    "    for i in range(data_orig.shape[0]):# 通过遍历修改data[i][2]的字符串\n",
    "        filtered_words = [word.lower() for word in data_orig[i][2].split() if word not in stopwords.words('english')]\n",
    "        l = ' '.join(filtered_words)\n",
    "        data_filtered[i][2] = ' '.join(filtered_words) \n",
    "    return data_filtered\n",
    "\n",
    "\n",
    "# test\n",
    "# d = np.array([[1,1,'A series of escapades demonstrating the a',1],\n",
    "#                [2,1, 'A series of escapades demonstrating the adage that what is good for the goose', 2],\n",
    "#                [3, 1, 'A series' ,2],\n",
    "#                [4, 1, 'A', 2],\n",
    "#                [5, 1, 'series', 2]])\n",
    "# d_f = remove_stopword(d)\n",
    "# d_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bbd3ab",
   "metadata": {},
   "source": [
    "## make dictionary=tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b64ec2c3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d2_bow:  {'goose': 0, 'a': 1, 'good': 2, 'adage': 3, 'escapades': 4, 'series': 5, 'demonstrating': 6}\n",
      "d2_ng:  {'escapades demonstrating adage': 0, 'goose': 1, 'a': 2, 'good goose': 3, 'a series escapades': 4, 'series escapades demonstrating': 5, 'demonstrating adage good': 6, 'adage good goose': 7, 'series': 8, 'a series': 9, 'demonstrating': 10, 'escapades demonstrating': 11}\n"
     ]
    }
   ],
   "source": [
    "# 之后要把这里的data改成X，不要整个data进入\n",
    "def make_dict_bow(data_filtered, text_col=2):\n",
    "    '''\n",
    "    make a bow dictionary based on all the words in the text\n",
    "    return a set that contains a word list mapped with numbers\n",
    "    \n",
    "    data: filtered data with all columns\n",
    "    text_col: text column index\n",
    "    '''\n",
    "    # make a non-repetitive word set\n",
    "    word_set = set()\n",
    "    for row in data_filtered:\n",
    "        for word in row[text_col].split():\n",
    "            word_set.add(word) # ('a','b')\n",
    "            \n",
    "    # map each string with an index number\n",
    "    word_size = len(word_set)\n",
    "    dict_bow = dict(zip(word_set,range(word_size))) # {'a':0,'b':1}\n",
    "                    \n",
    "    return dict_bow\n",
    "\n",
    "                    \n",
    "def make_dict_ng(data_filtered, text_col=2, ngram = 3):\n",
    "    '''\n",
    "    make a n-gram dict\n",
    "    \n",
    "    data_filtered: sentences array\n",
    "    ngram: n-gram number\n",
    "    '''\n",
    "    # \"在一般的文本分类任务中,N 取 3 就足够了\n",
    "    \n",
    "    # make a non-repetitive word set \n",
    "    word_set = set()\n",
    "    for row in data_filtered:\n",
    "        wordlist_row = row[text_col].split()\n",
    "        for n in range(ngram):\n",
    "            for i in range(len(wordlist_row) - n):\n",
    "                word_set.add(' '.join(wordlist_row[ i : i + ngram ])) # ('a','a b')\n",
    "        \n",
    "    # map each string with an index number\n",
    "    word_size = len(word_set)\n",
    "    dict_ng = dict(zip(word_set,range(word_size))) # {'a':0,'b':1}\n",
    "    \n",
    "    return dict_ng\n",
    "\n",
    "# test passed!\n",
    "# d2 = np.array([['1', '1', 'a series escapades demonstrating', '1'],\n",
    "#        ['2', '1', 'a series escapades demonstrating adage good goose',\n",
    "#         '2'],\n",
    "#        ['3', '1', 'a series', '2'],\n",
    "#        ['4', '1', 'a', '2'],\n",
    "#        ['5', '1', 'series', '2']], dtype='<U77')\n",
    "# print('d2_bow: ', make_dict_bow(d2,2))\n",
    "# print('d2_ng: ', make_dict_ng(d2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553b9b1",
   "metadata": {},
   "source": [
    "## data -> vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fa022bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: \n",
      " [['1' '1' 'a series escapades demonstrating' '1']\n",
      " ['2' '1' 'a series escapades demonstrating adage good goose' '2']\n",
      " ['3' '1' 'a series' '2']\n",
      " ['4' '1' 'a' '2']\n",
      " ['5' '1' 'series' '2']] \n",
      "\n",
      "{'goose': 0, 'a': 1, 'good': 2, 'adage': 3, 'escapades': 4, 'series': 5, 'demonstrating': 6} \n",
      " ************************* \n",
      " [[0. 1. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      "\n",
      "{'escapades demonstrating adage': 0, 'goose': 1, 'a': 2, 'good goose': 3, 'a series escapades': 4, 'series escapades demonstrating': 5, 'demonstrating adage good': 6, 'adage good goose': 7, 'series': 8, 'a series': 9, 'demonstrating': 10, 'escapades demonstrating': 11} \n",
      " ************************* \n",
      " [[0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def vectorize_bow(data_filtered, dict_bow, text_col=2): \n",
    "    '''\n",
    "    data_filtered: array, filtered data\n",
    "    return an array of vectorized features\n",
    "    '''\n",
    "    features_bow = np.zeros((data_filtered.shape[0],len(dict_bow)))\n",
    "    for i in range(data_filtered.shape[0]): # array中的每个句子：\n",
    "        phrase_i = data_filtered[i][text_col]\n",
    "        wordlist = phrase_i.split()\n",
    "        for word in wordlist: # for每个句子中的每个单词：\n",
    "            if word in dict_bow:# if这个单词在字典中：\n",
    "                j = dict_bow[word]\n",
    "                features_bow[i][j] += 1 # features对应dict相同index的位置变为1\n",
    "    return features_bow\n",
    "\n",
    "\n",
    "def vectorize_ng(data_filtered, dict_ng, text_col=2, ngram=3): # N-gram：ng_vectorizer.transform(corpus)\n",
    "    '''\n",
    "    data_filtered: array, filtered data\n",
    "    return an array of vectorized features\n",
    "    '''\n",
    "    features_ng = np.zeros((data_filtered.shape[0],len(dict_ng)))\n",
    "    for i in range(data_filtered.shape[0]): # for array中的每个句子\n",
    "        phrase_i = data_filtered[i][text_col]\n",
    "        wordlist = phrase_i.split()\n",
    "        for n in range(ngram):\n",
    "            for index in range(len(wordlist)-n): # for每个句子中的每个word_ng：\n",
    "                word_ng = ' '.join(wordlist[ index : index + n+1 ])\n",
    "                # print(index, word_ng)\n",
    "                if word_ng in dict_ng: # if这个word_ng在字典中：\n",
    "                    j = dict_ng[word_ng]\n",
    "                    features_ng[i][j] += 1 # features对应dict相同index的位置+1\n",
    "    return features_ng\n",
    "\n",
    "# test passed!\n",
    "# a3 = np.array([['1', '1', 'a series escapades demonstrating', '1'],\n",
    "#        ['2', '1', 'a series escapades demonstrating adage good goose',\n",
    "#         '2'],\n",
    "#        ['3', '1', 'a series', '2'],\n",
    "#        ['4', '1', 'a', '2'],\n",
    "#        ['5', '1', 'series', '2']], dtype='<U77')\n",
    "# print('original data: \\n',a3,'\\n')\n",
    "\n",
    "# dict_bow_t = make_dict_bow(a3,2)\n",
    "# dict_ng_t = make_dict_ng(a3)\n",
    "\n",
    "# f_t_bow = vectorize_bow(a3, dict_bow_t)\n",
    "# f_t_ng = vectorize_ng(a3, dict_ng_t)\n",
    "# print(dict_bow_t, '\\n', '*'*25, '\\n',f_t_bow)\n",
    "# print('\\n')\n",
    "# print(dict_ng_t, '\\n', '*'*25, '\\n', f_t_ng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14d744",
   "metadata": {},
   "source": [
    "## train (get model+gradient descent)\n",
    "\n",
    "cost function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{R}(w)\n",
    "\t&=-\\frac{1}{N}\\displaystyle \\sum^{N}_{n=1}\\mathcal{L}(y^{(n)},\\hat y^{(n)})\\\\\n",
    "\t&=-\\frac{1}{N}\\displaystyle \\sum^{N}_{n=1}{\\left(y^{(n)}\\right)^T\\log \\hat y^{(n)}\\\\}\n",
    "\\end{aligned}\\tag{3.6}\n",
    "$$\n",
    "\n",
    "gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{R}(w)}{\\partial w}=-\\frac{1}{N}\\sum_{n=1}^{N}{x^{(n)}(y^{(n)}-\\hat y^{(n)})}\\tag{3.7}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c8fbe71",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2523655359.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [69]\u001b[0;36m\u001b[0m\n\u001b[0;31m    后面参考ng就可以 和那个识别10011的task一样\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    '''compute the softmax output of z = wx + b'''\n",
    "    ez = np.exp(z)\n",
    "    sm = ez / np.sum(ez)\n",
    "    return sm\n",
    "    \n",
    "\n",
    "def compute_cost(X, y, W, b):\n",
    "    '''\n",
    "    compute the total cost of given parameters\n",
    "    \n",
    "    Args:\n",
    "        X (array, (m,n)): m training examples, n features\n",
    "        y (array, (n,))\n",
    "    '''\n",
    "    \n",
    "    m,n = X.shape\n",
    "    y_hat = np.dot(W,x) + b\n",
    "    cost = np.dot(y, np.log(y_hat)) * (-1) / m\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# 遗留问题：bias的处理？itertimes和epoch？mini-batch & sgd的实现方法？\n",
    "def train(X_train, y_train, W_init, b_init, alpha, itertimes, gd='batch'):\n",
    "    '''\n",
    "    learn the parameters\n",
    "    \n",
    "    Args:\n",
    "        X_train (array, (m,n)): m training examples, n features\n",
    "        y_train (array, (m,))\n",
    "        W_init (array, (n,))\n",
    "        alpha (float): learning rate\n",
    "        itertimes (int): iteration times\n",
    "        gd (string): method for gradient descent. 'batch', 'stochastic' or 'mini-batch'\n",
    "    Return\n",
    "        W (array, (n,)): parameters\n",
    "        cost_hist (list): a list of cost for plotting\n",
    "    '''\n",
    "    \n",
    "    m,n = X.shape\n",
    "    \n",
    "    # compute gradient\n",
    "    dj_dw = np.zeros((n,))                            #(n,)\n",
    "    dj_db = 0.                                        #scalar\n",
    "    for i in range(m):\n",
    "        y_hat = softmax(np.dot(X_train[i],W)+b_init)         #(n,)(n,)=scalar\n",
    "        err_i  = y_hat_i  - y[i]                             #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X_train[i,j]      #scalar\n",
    "        dj_db += err_i\n",
    "    dj_dw = dj_dw/m                                         #(n,)       \n",
    "    dj_db = dj_db/m  \n",
    "    \n",
    "    # gradient descent: SGD/batch/mini-batch\n",
    "    # 还差minibatch和sgd 要看一下书\n",
    "    cost_hist = []\n",
    "    W = copy.deepcopy(W_in)\n",
    "    for i in range(itertimes):\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        W = W - alpha * dj_dw\n",
    "        b_init = W - alpha * dj_db\n",
    "      \n",
    "        # Save cost at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost_hist.append(compute_cost(X_train, y_train, W, b_init) )\n",
    "    \n",
    "    return W, cost_hist\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177030e6",
   "metadata": {},
   "source": [
    "### #prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7843dc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bec4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data():\n",
    "\n",
    "\n",
    "\n",
    "def accuracy():\n",
    "\n",
    "def plot_result():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
